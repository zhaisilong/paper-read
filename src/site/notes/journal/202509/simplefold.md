---
{"dg-publish":true,"permalink":"/journal/202509/simplefold/"}
---


# arXiv 2025 | SimpleFold: 蛋白折叠，大道至简

![](https://cdn.molastra.com/weixin/2025/09/25/966282a4ecca85091dacf7fd31b7e436.jpg)

获取详情及资源:

- 论文: https://doi.org/10.1038/s42256-024-00860-4
- 代码: https://github.com/apple/ml-simplefold

## 0 摘要

蛋白质折叠模型的突破性成果，通常依赖于**将领域知识融入模型架构与训练流程**的组合方式。然而，随着生成模型在多个相关任务上的成功，便引出了一个自然的问题：这些复杂的架构设计是否真的是高性能模型的必要条件？

在这项研究中，提出了**SimpleFold**，这是首个基于**流匹配(flow-matching)的蛋白质折叠模型，并且完全由通用的Transformer模块构成**。传统的蛋白质折叠模型往往需要计算量庞大的组件，例如三角更新、显式的残基对表示，或是为该领域特别设计的多重训练目标。而SimpleFold仅依赖标准的Transformer模块，配合自适应层，并通过生成式的流匹配目标函数进行训练，同时引入一个额外的结构项。

研究将SimpleFold扩展到**30亿参数规模**，并在约**900万条蒸馏蛋白质结构数据**与实验PDB数据上进行训练。在标准折叠基准测试中，SimpleFold-3B取得了与当前最先进方法相当的性能，同时在**集成预测(ensemble prediction)任务上展现出强大优势——这一点对基于确定性重建目标训练的模型而言尤为困难。得益于其通用化的架构，SimpleFold在消费级硬件**上的部署与推理也表现出高效性。

总体而言，SimpleFold挑战了蛋白质折叠领域对复杂、特定架构的依赖，为未来的模型设计开辟了新的思路与空间。

![](https://cdn.molastra.com/weixin/2025/09/25/4ae1f167bba26ee037cf59ccb55b8cd6.jpg)

**图1 | SimpleFold在不同目标上的预测示例** (a) 7QSW的A链(大亚基RubisCO)，(b) 8DAY的A链(二甲基烯丙基色氨酸合酶1)，其中真实结构以浅青色表示，预测结构以深青色表示。(c) 基于MD集成数据微调的SimpleFold在6NDW的B链(鞭毛钩蛋白FlgE)上生成的构象集合。(d) SimpleFold在CASP14上的表现，随着模型规模从1亿到30亿参数逐步提升。(e) 不同规模的SimpleFold在消费级硬件上的推理耗时示例，即M2 Max 64GB Macbook Pro。

## 1 引言

蛋白质折叠，即根据氨基酸序列预测其三维原子结构，是计算生物学中一项**长期且极具挑战性**的任务，对药物发现等领域有着深远影响。该研究从**生成式建模视角**切入，不对蛋白结构的自然生成过程作过强假设，而是借鉴视觉生成模型（如文本生成图像、文本生成3D）的思路，将氨基酸序列视为“文本提示”，由生成模型直接输出全原子三维坐标。

传统折叠模型如AlphaFold2与RoseTTAFold依赖复杂且高成本的领域专用模块，例如多序列比对(MSA)、残基对表示及三角更新，这些设计旨在**硬编码现有结构生成知识**。然而研究表明，对于缺乏同源蛋白的孤儿蛋白，基于蛋白语言模型的策略甚至优于依赖MSA的方法。因此，该研究提出一种**彻底摆脱领域特定组件**的思路，转向通用的Transformer骨干架构，并通过流匹配(flow matching)生成目标进行端到端训练。

生成模型天然能够捕捉结构的不确定性，不仅可输出单一解构，还能生成**可行构象的集合**，更符合自然状态下的蛋白折叠过程。尽管近年来已有基于扩散或流的折叠尝试，但大多仍沿用了昂贵的AlphaFold2式模块。SimpleFold则在方法上实现了突破：**不依赖MSA、残基对表示、三角更新或几何等变模块**，而是基于标准Transformer与自适应层，直接将序列映射到三维结构。

主要贡献包括：

- 将蛋白质折叠重新表述为**条件生成任务**，提出SimpleFold——首个基于流匹配的Transformer折叠模型。
- 将模型规模扩展至30亿参数，并利用约900万条蒸馏结构与PDB实验数据进行训练。
- SimpleFold-3B在折叠基准测试中取得与依赖启发式设计的先进方法相当的性能，并在集成预测中表现突出。
- 发布从1亿参数到30亿参数的完整模型族，其中SimpleFold-100M在主流基准上恢复了约90%的性能，同时在消费级硬件上推理效率极高。

综上，**SimpleFold打破了蛋白质折叠模型必须依赖复杂架构设计的传统观念**，展现出一种更简洁而高效的替代路径。

## 2 SimpleFold

### 2.1 流匹配(Flow-Matching)预备知识

流匹配生成模型将生成过程视为一个**随时间演化的过程**，通过对常微分方程(ODE)的积分，将噪声逐步映射为数据。在时间区间 $t \in [0,1]$ 上，流匹配定义了一条概率分布路径 $p_t(x_t)$，使其能够将一个可处理的分布 $p_0$（如高斯分布）连续变换为任意复杂的数据分布 $p_D$。这种变换由一个流 $\psi_t$ 描述，即 $p_t = [\psi_t]∗p_0$，其中 $∗$ 表示推前算子(pushforward operator)。

在实际中，这一变换由一个**可学习的时间依赖速度场** $v_\theta(x_t, t)$ 参数化。生成过程通过积分如下ODE实现：

$$d x_t = v_\theta(x_t, t),dt,$$

其作用是将噪声逐渐映射为数据。

在训练时，采用**线性插值路径**(linear interpolant path)，也称为**校正流(rectified flow)**。具体做法是，在经验数据分布 $x \sim p_D$ 与噪声样本 $\varepsilon \sim \mathcal{N}(0,I)$ 之间建立插值：

$$x_t = t x + (1-t)\varepsilon \quad (2.1)$$

此时目标速度定义为 $v_t = x - \varepsilon$。在流匹配中，训练网络 $v_\theta$ 来逼近该目标，即通过 $l_2$ 回归损失实现：

$$\mathbb{E}\big[||v_\theta(x_t, t) - v_t||^2\big].$$

这一方法能够提供与真实(不可解)边际得分一致的梯度。已有研究表明，在高斯边际分布下，扩散模型与流匹配在超参数变换的意义下**等价**。

### 2.2 基于流匹配的折叠

SimpleFold将蛋白质折叠建模为一个**条件流匹配生成模型**，从噪声中生成结构，条件输入为氨基酸序列。这种“氨基酸序列→蛋白质结构”的生成方式，在概念上与计算机视觉中的“文本生成图像”或“文本生成3D”模型非常相似。

具体而言，假设蛋白含有 $N_a$ 个重原子，则在噪声 $\varepsilon$ 与真实原子坐标 $x$ 之间建立线性插值，其中 $\varepsilon, x \in \mathbb{R}^{N_a \times 3}$，条件输入为氨基酸序列 $s \in \mathbb{R}^{N_r}$，$N_r$ 为残基数量。与以往仅对Cα骨架进行流匹配建模的工作不同，SimpleFold生成的是**全原子构象**，包括主链与侧链。

**训练目标** 网络 $v_\theta$ 以氨基酸序列作为条件输入，预测目标速度场 $v_\theta(x_t, s, t)$。其流匹配目标函数定义为：

$$\mathcal{L}_{FM} = \mathbb{E}_{x,s,\varepsilon,t} \frac{1}{N_a} \big| v_\theta(x_t, s, t) - (x - \varepsilon) \big|^2 \quad (2.2)$$

其中 $x_t$ 为训练时采样得到的带噪结构(见公式2.1)。

此外，引入了一个**局部距离差异测试(Local Distance Difference Test, LDDT)损失**，用于度量生成结构 $\hat{x}(x_t)$ 与真实结构 $x$ 之间的原子对距离误差。训练时，$\hat{x}(x_t)$ 通过一步欧拉法近似得到：

$$\hat{x}(x_t) = x_t + (1-t),v_\theta(x_t, s, t)$$

LDDT损失定义为：

$$\mathcal{L}_{LDDT} = \mathbb{E}_{x,s,\varepsilon,t}\left[\frac{\sum_{i \neq j} \mathcal{1} (\delta_{ij}<C)\sigma\left(\lVert\delta_{ij}-\hat{\delta}^t_{ij}\rVert\right)}{\sum 1(\delta_{ij}<C)}\right] \quad (2.3)$$

其中 $\delta_{ij}=\lVert x_i-x_j \rVert$，$\hat{\delta}^t_{ij}=\lVert \hat{x}(x_t)_i-\hat{x}(x_t)_j\rVert$ 分别表示真实结构与预测结构中原子对的距离；$\sigma(\cdot)$ 为非线性函数，$C$ 为邻近原子的截断距离。

最终损失函数为流匹配项与LDDT项的加权组合：

$$\mathcal{L} = \mathcal{L}_{FM} + \alpha(t)\mathcal{L}_{LDDT} \quad (2.4)$$

其中 $\alpha(t)$ 是与时间步 $t$ 相关的权重，并会随训练阶段的不同而调整(见第4.1节)。

**时间步重采样** 为了提升训练效率，并促使模型生成更精细的结构，研究在时间步$t$的采样上采用了如下分布：

$$p(t) = 0.98,LN(0.8, 1.7) + 0.02,U(0, 1)$$

其中，$LN$ 表示逻辑正态分布(logistic-normal distribution)，$U$ 表示均匀分布。与图像生成中常见的时间步重采样方法不同（通常在流动过程的中间阶段，即 $t=0.5$ 附近采样更密集），该方法将采样权重偏向于更接近真实数据的时间步（即 $t=1$ 附近），这一策略与无条件生成中的发现类似。

这种偏移能够显著改善生成样本的质量，尤其是在对侧链原子等**精细结构**进行建模时效果突出。原因在于，蛋白质结构本身存在明显的**由粗到细的层级关系**：二级结构→Cα骨架→侧链。通过在接近数据流形的区域进行过采样，模型能够更好地学习并还原这些细致的原子位置。

关于LDDT损失与时间步重采样的更多细节可见附录C.1。

![](https://cdn.molastra.com/weixin/2025/09/25/ea037cc09eab18651fd59ff4686e6db0.jpg)

**图2 | SimpleFold架构概览。** 整体基于**通用的标准Transformer模块**，并结合自适应层构建。原子编码器、残基主干网络与原子解码器均共享相同的基础模块。该设计**避免了对成对表示与三角更新的依赖**，实现了更简洁高效的结构建模。

### 2.3 架构

自AlphaFold2提出以来，诸如**三角更新(triangle updates)以及单一表示与成对表示的显式交互建模等组件，几乎成为蛋白折叠模型的标准设计。但这些架构选择是否是构建高性能模型的必要条件**，仍然是一个开放性问题。与以往方法形成鲜明对比，**SimpleFold完全基于通用的Transformer模块**来搭建架构(见图5对比)。

如图2所示，SimpleFold包含三大模块：轻量化的**原子编码器与解码器**（对称设置，块数与隐藏维度一致）以及一个计算量更重的**残基主干网络**。所有模块均采用带有时间步$t$条件自适应层的标准Transformer结构。

原子编码器输入“带噪”的原子坐标$x_t$及其原子特征(如原子类型、电荷，见附录A)，通过傅里叶位置嵌入进行编码，输出原子token $a \in \mathbb{R}^{N_a \times d_a}$。在编码器中使用局部注意力掩码，使原子token只能关注残基邻域内的原子，从而保证局部性。接着进行**分组操作**：同一残基内的原子token经平均池化得到残基token $r \in \mathbb{R}^{N_r \times d_a}$ (见图6的分组/解组示意)。

与文本生成图像/3D的做法类似，模型引入**预训练蛋白语言模型(PLM)对氨基酸序列进行嵌入。所有实验均采用ESM2-3B**将序列$s$映射为逐残基的条件嵌入$e \in \mathbb{R}^{N_r \times d_e}$。随后，序列嵌入与残基token在通道维拼接后输入残基主干网络，该部分包含模型大部分参数和计算。接下来的**解组操作**将残基token投影回原子token：即按照残基的原子数量，将相同残基token广播到其包含的所有原子，并结合来自原子编码器的skip connection，用以区分同一残基中的不同原子。

最后，原子解码器更新原子token并输出预测速度场$\hat{v}_t$，其中同样应用局部注意力掩码。为提升性能与稳定性，所有Transformer模块采用现代实现，包括QK-normalization与SwiGLU替代标准前馈层。整体架构体现了蛋白质的层次性，遵循“细粒度–粗粒度–细粒度”的设计，以平衡效率与表现。

在位置编码上，残基主干采用**旋转位置嵌入(RoPE)**：在注意力块中，序列第$n$个残基的query与key向量旋转$e^{i\theta_n}$。在原子编码器与解码器中，则扩展为**四维轴向RoPE**。前三个维度来自参考构象的3D原子坐标(由基于规则的化学信息方法预测的氨基酸局部结构)，最后一维是对应残基token的索引。每个轴控制隐藏维度的四分之一旋转，从而同时对query与key施加编码。

与以往工作显著不同，SimpleFold**摒弃了显式成对表示**。不像AlphaFold2或ESMFold那样依赖昂贵的MSA嵌入或PLM注意力得分来初始化pair representation，SimpleFold只保留单序列表示，既不需要三角更新，效率也更高。并且，它没有使用等变架构来保证物理意义的几何约束，而是直接建立在**非等变Transformer**上。为了处理蛋白结构的旋转对称性，训练时采用**SO(3)数据增强**，通过随机旋转结构目标，并依赖模型自身去学习对称性。

### 2.4 采样

在推理阶段，给定氨基酸序列$s$，首先将原子坐标初始化为高斯噪声 $x_0 \sim \mathcal{N}(0, I)$，然后将学习得到的向量场从$t=0$积分到$t=1$，从而生成与输入序列对应的全原子结构。为实现随机生成，采用基于Langevin形式的随机微分方程(SDE)描述流动过程，并利用速度场$v_\theta$与得分函数$s_\theta$的等价关系：

$$s_\theta(x_t, s, t) = \frac{t v_\theta(x_t, s, t) - x_t}{1-t}$$

具体而言，采用Euler–Maruyama积分器：

$$dx_t = v_\theta(x_t, s, t),dt + \tfrac{1}{2}w(t),s_\theta(x_t, t, c),dt + \sqrt{\tau \cdot w(t)}, d\bar{W}_t \quad (2.5)$$

其中$w(t) > 0$为时间相关的扩散系数，$\bar{W}_t$为反向时间Wiener过程，$\tau$控制随机性的幅度。实验发现，设

$$w(t) = \frac{2(1-t)}{t+\eta}$$

并引入小常数$\eta$确保数值稳定，可获得最佳采样质量。本设定在所有实验中均为默认。与以往研究一致，$\tau$在**高精度精细结构生成**与**构象集合建模**之间起到平衡作用。

### 2.5 置信度模块

为更好地评估生成结构的质量，设计了一个**预测LDDT(pLDDT)模块**，输出逐残基的LDDT值(范围0–100)作为置信度分数。在折叠模型完全训练后，冻结其参数，再单独训练该模块(见图8)。训练过程中，实时采样结构$\hat{x}$并输入折叠模型，在$t=1$时通过自适应层获取最终残基token $r$。

pLDDT模块由4层标准Transformer块组成，不含自适应层，以$r$为输入，输出预测的pLDDT。按照已有方法，将真实LDDT离散为50个区间，训练目标为交叉熵损失。

### 2.6 基于蒸馏数据的训练

近年来，计算预测的蛋白质结构数量呈爆炸式增长，典型资源包括**AlphaFold蛋白质结构数据库(AFDB)与ESM宏基因组图谱**，其规模均已达到上亿条。近期提出的**AFESM**进一步整合了这两大资源，并将其中的蒸馏结构划分为约512万个非单例结构簇。尽管已有部分折叠模型利用蒸馏数据进行训练（通常通过自蒸馏实现），但相比目前公开可得的庞大数据规模，实际应用仍然有限。因此，**如何有效利用这些大规模蒸馏数据以训练更强大的折叠模型**，仍是一个未被充分研究的问题。

SimpleFold的训练采用三类数据源的混合：

1. **PDB实验数据**：约16万条结构，时间截断至2020年5月，与ESMFold保持一致。
2. **AFDB的SwissProt子集**：筛选平均pLDDT大于85且标准差小于15的样本，得到约27万条蒸馏结构。
3. **AFESM代表性结构**：为每个簇选择代表性结构，并过滤pLDDT大于0.8的样本，共约190万条。

除最大规模模型外，所有SimpleFold均基于以上三类数据进行训练，总量约200万条。

对于**SimpleFold-3B**，训练集进一步扩展为**AFESM-E**：在每个簇的代表性结构之外，额外随机采样最多10条平均pLDDT大于80的结构，总计约860万条蒸馏数据。结合PDB与SwissProt，这一更大规模的训练集充分发挥了大模型的容量优势，因此成为SimpleFold-3B的训练基座。

## 3 相关工作

**蛋白质折叠** 自AlphaFold2与RoseTTAFold提出以来，基于学习的方法在蛋白质折叠任务上取得了突破性成果，引发了持续研究。AlphaFold2引入了特定领域的网络模块，如**三角注意力**，并在架构设计上显式建模了单一表示与成对表示的交互，同时依赖MSA提取蛋白序列的进化信息，以贴近生物学专家对数据生成过程的理解。后续工作如OmegaFold与ESMFold用**预训练蛋白语言模型的嵌入**替代MSA，在推理效率与孤儿蛋白预测上表现尤为突出。部分研究也致力于通过高效实现加速AlphaFold2的模块，如FastFold与MiniFold。这类模型大多基于局部坐标框架的回归目标，而非直接建模全原子坐标，因此在**构象集成预测(ensemble generation)**方面缺乏多样性。

**蛋白质的流匹配方法** 生成模型，尤其是扩散与流匹配方法，因能生成高质量合理样本而被引入蛋白质折叠。AlphaFlow/ESMFlow通过在AlphaFold2或ESMFold上调优流匹配目标，展现了集成预测的优势。然而，这些方法并非从零开始设计的生成模型，而是依赖于预训练的、基于确定性回归目标训练的折叠模型。AlphaFold3及其一系列复现模型(如Boltz-1、Protenix、Chai-1)则采用扩散方法，用于蛋白复合物及生物分子相互作用的生成。此外，已有工作将扩散或流匹配方法用于**从头蛋白质结构生成**，如RFDiffusion、Genie-2、P(all-atom)等。但这些方法大多仍依赖AlphaFold系列的启发式架构设计，如昂贵的三角注意力与显式pair表示，有些甚至基于手工构造的等变扩散过程。Proteina尝试简化架构，但仍需显式pair表示，且仅限于Cα结构的生成。

在小分子体系中，MCF曾探索了利用通用Transformer骨干进行构象生成。相比之下，**SimpleFold**与以往折叠模型形成鲜明对照：它旨在用**通用的Transformer骨干网络**解决折叠问题，并通过数据直接学习结构生成过程中的对称性，而非依赖繁复的特定领域架构。

## 4 实验

### 4.1 实验设置

为验证所提框架在蛋白质折叠任务中的可扩展性，训练了不同规模的SimpleFold模型(100M、360M、700M、1.1B、1.6B与3B)。随着模型规模的增大，同时提升原子编码器、解码器及残基主干网络的深度与隐藏维度(具体参数见表5)。训练时，单GPU上会对同一蛋白复制$B_c$次并采样不同的时间步$t$，跨GPU累积来自$B_p$个不同蛋白的梯度，从而构成有效批量大小$B_c \times B_p$(设置见表6)。该策略在经验上比随机拼接蛋白构成batch更稳定，并能提升性能。

**预训练** 分为预训练与微调两个阶段，主要区别在于数据选择。预训练阶段使用尽可能大的数据集(约200万结构，3B模型约870万)，涵盖PDB、AFDB的SwissProt子集及AFESM。最大序列长度设为256，短序列不做填充，长序列裁剪为256残基。损失函数中$\alpha(t)=1$，即在整个流过程中都引入LDDT监督。批量大小对大多数模型为512，1.6B与3B模型则分别为1024与3072。优化器为AdamW，学习率0.0001，前5000步线性warmup。

**微调** 阶段在高质量数据(PDB与SwissProt子集)上进行，以提升生成结构的保真度。最大序列长度扩展至512，便于利用更大蛋白结构。为适配显存，批次中的$B_c$减半。损失权重设为

$$\alpha(t)=1+8\text{ReLU}(t-0.5)$$

即在接近真实数据($t=1$)时逐渐提升LDDT损失的权重，最高为5。优化器仍为AdamW，学习率保持0.0001。与预训练相同，在整个过程中对模型参数施加指数滑动平均(EMA)，衰减率0.999，这是流匹配生成模型的常见做法。

**pLDDT模块训练** 在SimpleFold完成预训练与微调后，冻结折叠模型参数，仅训练pLDDT模块。训练数据为PDB与SwissProt的组合，包含实验与高质量蒸馏结构。训练时$\alpha(t)=1$，由SimpleFold实时生成结构样本(200步推理，$\tau=0.3$)。最大序列长度为512，优化器为AdamW，学习率0.0001。

**表1 | 蛋白质折叠在CAMEO22(上)与CASP14(下)基准上的性能表现。** 各指标均报告所有样本的**均值/中位数**。其中，**橙色**表示基于回归目标训练的基线模型，**绿色**表示基于生成式目标(如扩散/流匹配或自回归)训练的基线模型，**蓝色**表示SimpleFold，其同样采用生成式目标进行训练，但**不依赖MSA**。

![](https://cdn.molastra.com/weixin/2025/09/25/72a8224af225edb18cd82b3bb54e0a68.jpg)

### 4.2 蛋白质折叠

研究者在两个被广泛采用的蛋白质结构预测基准上评估了SimpleFold：**CAMEO22**与**CASP14**。这两个基准都对折叠模型的**泛化性、鲁棒性以及原子级精度**提出了严格要求。CAMEO22包含183个目标结构，长度在100至750个残基之间；CASP14则是更具挑战性的盲测平台，此处选取了70个单链蛋白，长度在50至1000个氨基酸之间。在推理中，SimpleFold采用$\tau = 0.01$，经验上该设置带来最佳整体表现。

评估指标包括：**TM-score**与**GDT-TS**(全局拓扑相似性)、**LDDT**与**LDDT-Cα**(局部原子精度)、以及**RMSD**(原子坐标均方根偏差)。对于每个指标，报告均值与中位数。对全原子模型报告全部指标，而对仅预测骨架的模型则仅报告TM-score与GDT-TS。表1给出了在两个基准上的结果。

基线方法按序列信息获取方式分组：一类依赖**MSA搜索**(如RoseTTAFold、RoseTTAFold2、AlphaFold2)，另一类使用**预训练蛋白语言模型(PLM)嵌入**(如ESMFold与OmegaFold)。此外，还区分是否采用生成式目标(扩散、流匹配或自回归)，而非直接回归到真实结构。例如，AlphaFlow与ESMFlow即为基于AlphaFold2和ESMFold微调的流匹配模型。值得注意的是，这些微调模型在全部指标上反而落后于原始的回归模型，原因在于CAMEO22与CASP14通常仅提供单一“真实”结构，从而更有利于点对点预测的回归模型。

尽管架构更为简洁，SimpleFold依然取得了与现有基线相当甚至更优的表现。在CAMEO22上，SimpleFold表现接近最优模型(如ESMFold、RoseTTAFold2与AlphaFold2)，在多数指标上达到后两者的95%以上，但无需昂贵的三角注意力与MSA。在更具挑战性的CASP14中，SimpleFold表现甚至超过了ESMFold。例如，SimpleFold-3B的TM-score为0.720/0.792、GDT-TS为0.639/0.703，而ESMFold对应结果为0.701/0.792与0.622/0.711。同时，SimpleFold在性能上接近甚至超过一些依赖MSA的模型如RoseTTAFold与AlphaFlow。值得注意的是，除了AlphaFold2外，所有模型在CASP14上的表现都相较CAMEO22有明显下降，但SimpleFold的性能下降幅度更小，显示出其在复杂任务上的**鲁棒性**。

进一步分析不同规模模型的表现发现：最小的SimpleFold-100M在保持高效率的同时，性能已能达到ESMFold在CAMEO22上的90%以上，证明了通用Transformer架构的有效性。而随着模型规模的扩大，性能整体上持续提升，尤其在CASP14这一更复杂的基准上提升更为显著。这为**通用架构在大规模下的潜力**提供了有力实证。

**表2 | 在MD构象集上的评估结果。** 基线模型的结果取自(Jing et al., 2024a; Lu et al., 2024a)，SimpleFold(SF)与SimpleFold-MD(SF-MD)遵循相同的评估流程。

![](https://cdn.molastra.com/weixin/2025/09/25/bb8a45e3e658e9451fedcae6e9b85419.jpg)

### 4.3 基于pLDDT的置信度评估

图3(a)展示了一个带有pLDDT评分的预测结构，其中红色与橙色表示低pLDDT，蓝色表示高pLDDT。如图所示，SimpleFold在**二级结构区域的预测具有较高置信度**，而在柔性环(loop)区域则表现出不确定性。图3(b)与(c)对比了pLDDT与真实LDDT-Cα的关系，评估对象包括CAMEO22的目标以及自2023年1月以来从PDB随机选取的1000条蛋白链。结果表明，pLDDT与LDDT-Cα之间的**Pearson相关系数达到0.77**，说明SimpleFold的pLDDT模块能够较好地刻画预测结构的整体质量。

需要强调的是，pLDDT模块的输出并不依赖于生成流过程，因此**它也可以无缝应用于其他模型的预测质量评估**。这一方向将在未来研究中进一步探索。

![](https://cdn.molastra.com/weixin/2025/09/25/c44fb94bf1506e9e88182c82dcbfea65.jpg)

**图3** (a) SimpleFold预测结构及其pLDDT评分示例（颜色从红到深蓝分别表示pLDDT由低到高，可视化方式参考Chakravarty and Porter (2022)）。(b)与(c) 对比了pLDDT与LDDT-Cα的关系。

### 4.4 集成生成

#### 4.4.1 分子动力学集成

由于采用了**生成式训练目标**，SimpleFold天然能够建模蛋白质结构的分布，不仅能针对给定氨基酸序列生成一个确定性结构，还能生成**不同构象的集成**。为验证这一能力，研究者在**ATLAS数据集**上进行了基准测试。ATLAS包含1390个蛋白的全原子分子动力学(MD)模拟。按照AlphaFlow的划分方法，将数据集分为训练、验证与测试集，并在测试集上对每个蛋白生成250个构象。表2比较了SimpleFold与基线模型在ATLAS上的表现(不同规模SimpleFold结果见表9)。评估指标涵盖了灵活性预测(RMSDᵣ与RMSFᵣ)、分布精度(RMWD)以及集成可观测量(如暴露残基与暴露MI矩阵)。

首先，直接评估未经MD数据微调的**SimpleFold-3B**。在推理中设$\tau=0.6$，比折叠任务中引入更多随机性。与AlphaFold2及MSA subsampling基线相比，SimpleFold在生成符合MD模拟分布的集成结构上表现更优。需要注意的是，ESMFold基于确定性回归目标训练，若不进行额外调优无法用于集成生成。

此外，还训练了**SimpleFold-MD**，即在ATLAS训练集上对完整训练过的SimpleFold额外微调2万步(损失中$\alpha(t)=1$)。研究者将其与同样经过额外调优的基线(ESMDiff、ESMFlow-MD与AlphaFlow-MD)进行比较。结果表明，SimpleFold在多数指标上**持续优于ESMFlow-MD**，两者都依赖ESM嵌入而不使用MSA。同时，SimpleFold在集成可观测量相关指标上也**优于AlphaFlow-MD**，而这些指标在药物发现中识别隐性结合口袋时至关重要。

**表3 | 双态构象结果。** 对于最后两个指标，同时报告所有目标的**均值与中位数**。基线结果取自ESMDiff论文(Lu et al., 2024a)，其余模型的评估流程保持一致。

![](https://cdn.molastra.com/weixin/2025/09/25/568dea0e95b210d086e64481641d69f8.jpg)

#### 4.4.2 多态结构预测

研究者进一步评估了SimpleFold在生成具有多种天然构象的蛋白结构方面的能力。采用的基准包括**apo-holo构象变化数据集**(Saldaño et al., 2022)以及**fold-switchers数据集**(Chakravarty and Porter, 2022)，评估流程与EigenFold (Jing et al., 2023)保持一致。每个数据集中的目标由(1)氨基酸序列和(2)两种不同的真实结构构成，要求模型能够生成一组**多样且准确的样本**，既能覆盖两种构象状态，又能反映正确的局部柔性。

对比方法包括：(1) **基于序列的方法**：FoldFlow2、MultiFlow、Str2Str、EigenFold、ESMDiff、ESMFlow；(2) **基于MSA的方法**：MSA subsampling与AlphaFlow。评估指标包括**全局与逐残基柔性(res. flex.)**，以及**集成TM分数(TM-ens)**。具体评估流程为：对每个目标生成5个样本，并分别与两种真实构象进行比较以计算指标。在推理中，将SimpleFold的$\tau$设为0.8，以生成既能对齐两种天然构象又能正确建模残基柔性的结构。

结果如表3所示，SimpleFold在**Apo/holo数据集**上取得了当前最优表现，显著超过了强基线方法AlphaFlow等MSA类模型；在**Fold-switch数据集**上，SimpleFold表现与ESMFlow相当甚至更优，而ESMFlow同样采用流匹配目标并基于ESM嵌入。结果验证了SimpleFold在**高质量结构生成(集成TM分数)**以及**结构柔性建模(残基柔性)**上的能力。同时，随着模型规模的增加，SimpleFold整体性能进一步提升，展示了该框架在蛋白质集成生成上的潜力。

综上，在**MD构象集**与**多态结构基准**上的实验结果均表明，SimpleFold能够有效建模蛋白质结构的集成，这对于需要蛋白柔性建模的应用(如分子对接)具有重要价值。

![](https://cdn.molastra.com/weixin/2025/09/25/45910d9736c7cab120f26846ba603643.jpg)

**图4 | SimpleFold的可扩展性表现。** (a)(b) 展示训练计算量(Gflops)与折叠性能(GDT-TS、TM-score)的关系。(c)(d) 展示训练步数与折叠性能(GDT-TS、TM-score)的关系。(e)(f) 展示数据规模对性能(GDT-TS、TM-score)的影响。所有模型均在CAMEO22基准上进行评测。

### 4.5 蛋白质折叠中的规模效应

SimpleFold在扩展模型规模时获益显著，这与生成模型在视觉、语言等领域的成功经验相一致。然而，在蛋白质折叠任务中，**训练数据与模型规模扩展的效果尚未得到系统研究**。本节通过实证分析，从模型规模与数据规模两方面展示了SimpleFold的扩展特性，并强调了构建强大生物生成模型的重要考量。

**模型规模扩展** 为评估模型大小的作用，训练了从1亿参数(SimpleFold-100M)到30亿参数(SimpleFold-3B)的一系列模型。所有模型均使用完整的预训练数据(PDB、AFDB的SwissProt以及筛选后的AFESM)。结果如图4(a)-(d)所示，模型规模越大、训练预算(计算量与迭代步数)越充足，性能提升越明显。这表明SimpleFold展现出**良好的正向扩展趋势**，为进一步发展更强大的生物生成模型提供了方向。

**数据规模扩展** 进一步使用SimpleFold-700M考察训练数据规模的影响，依次采用：(1) PDB(16万条结构)，(2) PDB+AFDB的SwissProt(27万条结构)，(3) PDB+SwissProt+AFESM代表性结构(190万条)，(4) 扩展AFESM-E(约860万条结构)。如图4(e)(f)所示，随着数据源多样化与结构数量增加，SimpleFold在40万步训练后的性能逐步提升。

综上，这些结果支持了SimpleFold的核心贡献：**通过简化且可扩展的架构，充分利用不断增长的实验与蒸馏蛋白数据，实现性能持续提升**。

## 5 结论与未来工作

该研究提出了**SimpleFold**，一种基于流匹配的蛋白质折叠生成模型，代表着与以往方法架构设计的**显著背离**。SimpleFold完全由通用的Transformer模块与自适应层构成，摒弃了AlphaFold2中昂贵的成对表示与三角更新等启发式设计；训练目标也仅由**简洁的流匹配损失与附加的LDDT损失**组成，而非多种复杂的蛋白质专用损失函数。这一简化框架使得SimpleFold能够在**模型规模与数据规模上实现扩展**。其中最大模型SimpleFold-3B在标准折叠任务中表现出与现有方法竞争的性能，同时凭借生成式训练目标，在多个集成生成任务中达到甚至超越了最新水平。SimpleFold是首个在蛋白质折叠中**系统展现良好扩展特性**的工作，凸显了通过大幅简化架构即可实现高效折叠预测的潜力，减少了对计算复杂网络模块的依赖。

由于代码与模型检查点已公开，SimpleFold有望被应用与扩展至更多蛋白质相关任务。其**简化架构**基于标准Transformer模块，便于结合常见微调方法(如Adapter与LoRA)在特定蛋白结构数据与下游任务中发挥作用。此外，SimpleFold也可通过蒸馏加速推理与提升3B大模型的部署效率。除大规模模型外，研究者还发布了轻量级版本SimpleFold-100M，其具备更高效的推理速度，适用于推理延迟受限的场景。

总而言之，SimpleFold展现了以**通用架构扩展学习底层对称性**的新范式，有望成为推动构建高效且强大的蛋白质生成模型的重要起点。

